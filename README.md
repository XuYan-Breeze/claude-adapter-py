# Claude Adapter Python

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

[English](#overview) | [ä¸­æ–‡](README_CN.md)

---

## Overview

Claude Adapter Python is a local HTTP proxy server that lets you use **OpenAI-compatible APIs** with [**Claude Code**](https://github.com/anthropics/claude-code).

It translates Anthropic Messages API requests to OpenAI Chat Completions format, so you can:

- Use any OpenAI-compatible API with Claude Code
- Easily switch between different AI providers
- Run local models with Ollama or LM Studio
- Full tool calling support, both native and XML modes
- Streaming responses for real-time interaction

## Supported Providers

| Category | Provider | Type | Description |
|----------|----------|------|-------------|
| **Free** | NVIDIA NIM | Cloud API | https://build.nvidia.com |
| **Free** | Ollama | Local + Cloud | https://ollama.com, supports local and cloud models |
| **Free** | LM Studio | Local only | https://lmstudio.ai, local models only |
| **Paid** | Kimi | Cloud API | https://platform.moonshot.cn |
| **Paid** | DeepSeek | Cloud API | https://platform.deepseek.com |
| **Paid** | GLM Z.ai | Cloud API | https://bigmodel.cn |
| **Paid** | MiniMax | Cloud API | https://platform.minimaxi.com |
| **Custom** | OpenAI-compatible | Any | Any OpenAI-compatible endpoint |

## Installation

```bash
# From source
git clone <repo-url>
cd claude-adapter-py
pip install -e .

# Or from PyPI when published
pip install claude-adapter-py
```

## Quick Start

### 1. Run the adapter

```bash
claude-adapter-py
```

### 2. Select provider type

The CLI will show three categories:

```
? Choose provider type:
  ðŸ†“ Free    NVIDIA, Ollama, LM Studio
  ðŸ’° Paid    Kimi, DeepSeek, GLM, MiniMax
  ðŸ”§ Custom  OpenAI-compatible endpoint
```

### 3. Select specific provider

After choosing a category, pick the provider:

```
? Choose provider:
  NVIDIA NIM              NVIDIA NIM API (https://build.nvidia.com/)
  Ollama                  Ollama localhost:11434 (https://ollama.com/)
  LM Studio               LM Studio localhost:1234 (https://lmstudio.ai/)
```

### 4. Use saved config or configure

**If a saved config exists** for the selected provider:

```
? NVIDIA NIM found, choose action:
  â–¶  Use saved config  ä½¿ç”¨å·²å­˜å‚¨çš„ NVIDIA NIM é…ç½®å¯åŠ¨
  ðŸ”§ Reconfigure  é‡æ–°é…ç½® NVIDIA NIM å‚æ•°
```

**If no saved config exists**:

```
? No config for Ollama, choose action:
  ðŸ”§ Configure  é…ç½® Ollama å‚æ•°
  â†©  Go back  è¿”å›žé‡æ–°é€‰æ‹©
```

### 5. The adapter will

- Show setup guidance specific to your provider
- Walk you through API key, base URL, port, model mappings, tool format
- Save the configuration for future use
- Start the HTTP server on `http://localhost:3080`
- Update `~/.claude/settings.json` automatically

### 6. Use Claude Code normally

All requests will be routed through the adapter.

```bash
# Copy this to set the environment variable:
export ANTHROPIC_BASE_URL="http://localhost:3080"
```

---

## Provider Setup Guides

### NVIDIA NIM  Free, Cloud

1. Visit https://build.nvidia.com and sign up
2. Get your API Key, format: `nvapi-xxxx`
3. Choose a model, recommended: `minimaxai/minimax-m2.1`
4. Configure and start

### Ollama  Free, Local + Cloud

Ollama supports both **local models** and **cloud models**.

```bash
# 1. Install Ollama
#    Visit https://ollama.com/download

# 2. Start the service
ollama serve

# 3. Pull a local model
ollama pull qwen2.5-coder:32b

# 3b. Or pull a cloud model
ollama pull kimi-k2.5:cloud

# 4. Check available models
ollama list
```

> Make sure `ollama serve` is running before starting the adapter.

### LM Studio  Free, Local only

LM Studio **only supports local models**. You must download, load, and serve before use.

```bash
# 1. Download LM Studio from https://lmstudio.ai

# 2. Download a model
lms get <model-name>

# 3. Load the model into memory
lms load <model-name>

# 4. Start the server
lms server start
```

> The server runs on port 1234 by default. Increase Context Length to 16384+ in LM Studio settings.

### Kimi  Paid, Cloud

1. Visit https://platform.moonshot.cn/console/api-keys
2. Sign up and create an API Key, format: `sk-xxxx`
3. Recommended model: `kimi-k2.5`

### DeepSeek  Paid, Cloud

1. Visit https://platform.deepseek.com/api_keys
2. Sign up and create an API Key, format: `sk-xxxx`
3. Recommended model: `deepseek-chat`

### GLM Z.ai  Paid, Cloud

1. Visit https://bigmodel.cn/usercenter/proj-mgmt/apikeys
2. Sign up and create an API Key, format: `xxxx.xxxx`
3. Recommended model: `glm-4.7`

### MiniMax  Paid, Cloud

1. Visit https://platform.minimaxi.com/user-center/basic-information/interface-key
2. Sign up and create an API Key, format: `eyxxxx`
3. Recommended model: `MiniMax-M2.1`

### Custom OpenAI-compatible

1. Prepare any OpenAI-compatible API endpoint
2. Enter the Base URL, e.g. `https://api.openai.com/v1`
3. Enter your API Key
4. Enter the model name

---

## CLI Commands

```bash
# Start server, interactive provider selection
claude-adapter-py

# Force reconfigure current provider
claude-adapter-py -r

# Custom port
claude-adapter-py -p 8080

# Skip Claude settings update
claude-adapter-py --no-claude-settings

# List saved providers
claude-adapter-py ls

# Remove a provider config
claude-adapter-py rm <provider-name>

# Show help
claude-adapter-py -h
```

## Configuration

All configs are stored in `~/.claude-adapter/`:

```
~/.claude-adapter/
â”œâ”€â”€ settings.json           # Active provider
â”œâ”€â”€ metadata.json           # User metadata
â”œâ”€â”€ providers/              # Per-provider configs
â”‚   â”œâ”€â”€ nvidia.json
â”‚   â”œâ”€â”€ ollama.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ token_usage/            # Daily token usage logs
â”‚   â””â”€â”€ 2026-02-10.jsonl
â””â”€â”€ error_logs/             # Error logs
    â””â”€â”€ 2026-02-10.jsonl
```

### Example provider config

```json
{
  "provider": "ollama",
  "base_url": "http://localhost:11434/v1",
  "api_key": "ollama",
  "models": {
    "opus": "qwen2.5-coder:32b",
    "sonnet": "qwen2.5-coder:14b",
    "haiku": "qwen2.5-coder:7b"
  },
  "tool_format": "native",
  "port": 3080,
  "max_context_window": 8192
}
```

## Architecture

```
Claude Code  ->  Anthropic API request
                     |
              Claude Adapter  localhost:3080
                     |
              Convert to OpenAI format
                     |
              OpenAI-compatible API  NVIDIA, Ollama, etc.
                     |
              Convert back to Anthropic format
                     |
              Claude Code  receives response
```

## Tool Calling Modes

### Native mode  Recommended for cloud APIs

Uses OpenAI native function calling. Best for NVIDIA, Kimi, DeepSeek, GLM, MiniMax, and cloud providers.

### XML mode  Recommended for local models

Injects XML tool instructions into the system prompt. Models output `<tool_code>` XML tags. Better for local models without native function calling support.

## Troubleshooting

### Port already in use

The adapter auto-finds the next available port, or specify one:

```bash
claude-adapter-py -p 8080
```

### Context window errors with LM Studio or Ollama

- Increase context length in LM Studio GUI to 16384+
- Or edit `~/.claude-adapter/providers/lmstudio.json` and set `"max_context_window": 32768`

### API key issues

```bash
claude-adapter-py -r
```

### Model not found

**Ollama**: run `ollama list` and `ollama pull <model>`

**LM Studio**: make sure the model is loaded with `lms load <model>` and server is running with `lms server start`

## Development

```bash
pip install -e ".[dev]"
pytest
black src/ tests/
ruff check src/ tests/
mypy src/
```

## Project Structure

```
claude-adapter-py/
â”œâ”€â”€ src/claude_adapter/
â”‚   â”œâ”€â”€ __init__.py         # Package init
â”‚   â”œâ”€â”€ __main__.py         # Entry point
â”‚   â”œâ”€â”€ cli.py              # CLI implementation
â”‚   â”œâ”€â”€ server.py           # FastAPI server
â”‚   â”œâ”€â”€ providers.py        # Provider presets and categories
â”‚   â”œâ”€â”€ models/             # Pydantic data models
â”‚   â”œâ”€â”€ converters/         # Protocol converters
â”‚   â”œâ”€â”€ handlers/           # API handlers
â”‚   â””â”€â”€ utils/              # Utilities
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ README_CN.md
```

## License

MIT License

## Credits

Python rewrite of the TypeScript [claude-adapter](https://github.com/shantoislamdev/claude-adapter) with enhanced features, multi-provider support, and improved architecture.
